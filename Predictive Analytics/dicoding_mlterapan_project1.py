# -*- coding: utf-8 -*-
"""Dicoding_MLTerapan_Project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-6xf5aFtQAjoec9yIfvCkztywSUvCgT4

# Predictive Maintenance By Sukron Chafidhi

![worker-operating-industrial-machine-metal-workshop.jpg](https://res.cloudinary.com/da0hsihog/image/upload/v1688784159/Portofolio/worker-operating-industrial-machine-metal-workshop_xs2cqt.jpg)
Source by <a href="https://www.freepik.com/free-photo/worker-operating-industrial-machine-metal-workshop_11035743.htm#query=milling%20machine&position=29&from_view=search&track=ais">Image by aleksandarlittlewolf</a> on Freepik

List of Content:
*   [Data Understanding](scrollTo=gcKLFjESZ43X&line=1&uniqifier=1)
  * [Data Loading](#scrollTo=CN6CfDdEaLdk&line=1&uniqifier=1)
  * [Exploratory Data Analysis - Variable Description](#scrollTo=UJ6GRaNuL1s0&line=1&uniqifier=1)
  * [Exploratory Data Analysis - Handle Missing Values and Outliers](#scrollTo=ldk-QI4DsNdq&line=2&uniqifier=1)
  * [Exploratory Data Analysis - Multivariate Analysis](#scrollTo=nd1HvjZ9vSyZ&line=2&uniqifier=1)
*   [Data preparation](#scrollTo=Yw926e8Sz9JS&line=1&uniqifier=1)
*   [Modeling](#scrollTo=QEslsxjs3ckn&line=6&uniqifier=1)
*   [Evaluation](#scrollTo=TFvEsQfgcnTV&line=5&uniqifier=1)
"""

# hubungkan dengan gdrive
from google.colab import drive
drive.mount('/content/drive')

# install library kaggle
! pip install kaggle

# buat folder kaggle
! mkdir ~/.kaggle

# copy data api key kaggle ke folder kaggle
! cp "/content/drive/MyDrive/ML AI DL/kaggle.json" ~/.kaggle/

# mengizinkan owner untuk mengedit dan membaca file kaggle.json
! chmod 600 ~/.kaggle/kaggle.json

# mendownload dataset lewat api kaggle
! kaggle datasets download stephanmatzka/predictive-maintenance-dataset-ai4i-2020

# unzip dataset ke folder /content/datasets
! unzip predictive-maintenance-dataset-ai4i-2020.zip -d datasets

"""## Data Understanding
*   Memberikan informasi seperti jumlah data, kondisi data, dan informasi mengenai data yang digunakan
*   Menuliskan tautan sumber data (link download). https://www.kaggle.com/datasets/stephanmatzka/predictive-maintenance-dataset-ai4i-2020
*   Menguraikan seluruh variabel atau fitur pada data.
*   Melakukan beberapa tahapan yang diperlukan untuk memahami data contohnya teknik visualisasi data atau exploratory data analysis.

### Data Loading
Memuat data untuk diproses
"""

# Commented out IPython magic to ensure Python compatibility.
# import library
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})

# load dataset
data  = pd.read_csv('datasets/ai4i2020.csv')
data

"""### Exploratory Data Analysis - Variable Description
This synthetic dataset is modeled after an existing milling machine and consists of 10 000 data points from a stored as rows with 14 features in columns
1.   UID: unique identifier ranging from 1 to 10000
2.   Product ID: consisting of a letter L, M, or H for low (50% of all products), medium (30%) and high (20%) as product quality variants and a variant-specific serial number
3.   Type: just the product type L, M or H from column 2
4.   Air temperature [K]: generated using a random walk process later normalized to a standard deviation of 2 K around 300 K
5.   Process temperature [K]: generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.
6.  Rotational speed [rpm]: calculated from a power of 2860 W, overlaid with a normally distributed noise
7.  Torque [Nm]: torque values are normally distributed around 40 Nm with a SD = 10 Nm and no negative values.
8.  Tool wear [min]: The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process.
9.  A 'machine failure' label that indicates, whether the machine has failed in this particular datapoint for any of the following failure modes are true.

The machine failure consists of five independent failure modes.
1. tool wear failure (TWF): the tool will be replaced of fail at a randomly selected tool wear time between 200 - 240 mins (120 times in our dataset). At this point in time, the tool is replaced 69 times, and fails 51 times (randomly assigned).
2. heat dissipation failure (HDF): heat dissipation causes a process failure, if the difference between air- and process temperature is below 8.6 K and the tools rotational speed is below 1380 rpm. This is the case for 115 data points.
3. power failure (PWF): the product of torque and rotational speed (in rad/s) equals the power required for the process. If this power is below 3500 W or above 9000 W, the process fails, which is the case 95 times in our dataset.
4. overstrain failure (OSF): if the product of tool wear and torque exceeds 11,000 minNm for the L product variant (12,000 M, 13,000 H), the process fails due to overstrain. This is true for 98 datapoints.
5. random failures (RNF): each process has a chance of 0,1 % to fail regardless of its process parameters. This is the case for only 5 datapoints, less than could be expected for 10,000 datapoints in our dataset.
If at least one of the above failure modes is true, the process fails and the 'machine failure' label is set to 1. It is therefore not transparent to the machine learning method, which of the failure modes has caused the process to fail.






"""

# cek jumlah data
# minimum 500 sampel data
data.shape

# cek informasi dataset
data.info()

"""
Informasi yang didapat dari hasil diatas antara lain:

*   Terdapat 2 kolom dengan tipe object, yaitu: Product ID, dan Type. Kolom ini merupakan categorical features (fitur non-numerik).
*   Terdapat 9 kolom bertipe Int64, yaitu: UDI, Rotational speed [rpm], Tool wear [min], Machine failure, TWF, HDF, PWF, OSF, dan RNF. Kolom UDI merupakan identifier unik untuk masing-masing sensor, sementara Rotational speed [rpm], Tool wear [min] merupakan fitur numerik hasil dari pengukuran sensor, dan Machine failure merupakan target yang ingin diprediksi dengan Failure Category antara lain TWF, HDF, PWF, OSF, dan RNF.
*  Terdapat 3 kolom bertipe Float, yaitu: Air temperature [K], Process temperature [K] dan Torque [Nm]. Kolom tersebut merupakan fitur numerik hasil pembacaan sensor.

"""

# convert semua tipe data ke float
for column in data.columns:
    try:
        data[column]=data[column].astype(float)
    except:
        pass

# cek deskripsi statistik data
data.describe(include='all').T

# rename agar nama menjadi lowercase dan tidak ada whitespace
for column in data:
    try:
      data = data.rename(
        columns = {column : column.lower()
        .replace(" ", "_")
        .replace("[", "")
        .replace("]", "")})
    except:
      pass

# cek deskripsi statistik data untuk kolom bertipe data numerik
data.select_dtypes(include=[np.number]).describe(include='all').T

"""### Exploratory Data Analysis - Missing Value dan Outliers
Karena data tidak memiliki *missing value* maka tidak akan diproses, sementara *outliers* tidak akan dilakukan pada dataset, mengingat setelah saya coba lakukan ternyata data machine failure hanya menyisakan sedikit informasi.
"""

# cek duplikat
print('Cek data duplikat: ')
print('Jumlah data duplikat: ', data.duplicated().sum())
print('\n')

# cek missing value
print('Cek missing value')
data.isnull().sum()

"""### Exploratory Data Analysis - Multivariate analysis
 Kita akan menggambarkan grafik hubungan antar variabel dari data, untuk mengetahui nilai korelasi sehingga dapat menghilangkan fitur yang kurang berpengaruh.
"""

# install library
! pip install pandas_profiling

# import library
from pandas_profiling import ProfileReport

# Commented out IPython magic to ensure Python compatibility.
# # menampilkan grafik dengan pandas profiling
# %%time
# profile = ProfileReport(data,
#                         title="Predictive Maintenance",
#                         dataset={"description": "This profiling report was generated for Sukron Chafidhi",
#                                  "copyright_holder": "Sukron Chafidhi",
#                                 },
#                         explorative=True,
#                        )
# profile

"""Fitur yang akan dihilangkan pertama kali adalah udi dan product_id yang merupakan nama unik untuk mesin. Sementara itu, karena kita hanya mem-prediksi machine_failure sehingga kategori dari fitur itu tidak akan digunakan, antara lain kategori twf, hdf, pwf, osf, dan rnf. Terakhir, untuk menghilangkan fitur selanjutnya perlu diketahui nilai korelasi antar fitur ke fitur machine_failure, dan ternyata terdapat 2 fitur dengan nilai korelasi dibawah 0.05 atau berpengaruh 5% yang biasa digunakan untuk menilai signifikansi pada statistik yaitu type (3,4%). Jadi fitur yang akan dihilangkan antara lain udi, product_id, twf, hdf, pwf, osf, rnf, dan type."""

# menghapus fitur
data.drop(['udi', 'product_id', 'twf','hdf','pwf','osf','rnf', 'type'],axis=1,inplace=True)

data

data.describe(include='all').T

# ubah data kategorikal
data = pd.get_dummies(data,drop_first=True)

# buat array untuk mengelompokkan data kategorikal dan numerikal
numerical_features = ['air_temperature_k','process_temperature_k', 'rotational_speed_rpm', 'torque_nm', 'tool_wear_min']
categorical_features = ['machine_failure']

"""## Data Preparation
- Menerapkan dan menyebutkan teknik data preparation yang dilakukan.
- Teknik yang digunakan pada notebook dan laporan harus berurutan.
- Menjelaskan proses data preparation yang dilakukan
- Menjelaskan alasan mengapa diperlukan tahapan data preparation tersebut.
"""

data.head()

"""### Reduksi dimensi dengan Principal Component Analysis (PCA)
Karena fitur air_temperature_k dan process_temperature_k memiliki satuan yang sama dan saling berkorelasi, maka PCA dapat diterapkan untuk mereduksinya dan menjadi fitur baru berupa degree atau suhu.
"""

sns.pairplot(data[['air_temperature_k','process_temperature_k']], plot_kws={"s": 3});

# mereduksi fitur
from sklearn.decomposition import PCA
pca = PCA(n_components=1, random_state=123)
pca.fit(data[['air_temperature_k','process_temperature_k']])
data['degree'] = pca.transform(data.loc[:, ('air_temperature_k','process_temperature_k')]).flatten()
data.drop(['air_temperature_k','process_temperature_k'], axis=1, inplace=True)

data.head()

"""### Split Train Test
Membagi dataset ke dalam train dan split dengan presentasi 80:20
"""

# memperbarui data kategorikal
numerical_features = ['degree', 'rotational_speed_rpm', 'torque_nm', 'tool_wear_min']
# split data train test
from sklearn.model_selection import train_test_split

X = data.drop(["machine_failure"],axis =1)
y = data["machine_failure"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""### Normalization"""

# Normalisasi data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Normalize Training Data
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
print('Normalized Training Data\n')
X_train[numerical_features].head()

# Normalize Testing Data
scaler.fit(X_test[numerical_features])
X_test[numerical_features] = scaler.transform(X_test.loc[:, numerical_features])
print('Normalized Testing Data\n')
X_test[numerical_features].head()

"""## Modelling
- Membuat model machine learning untuk menyelesaikan permasalahan.
- Menjelaskan tahapan dan parameter yang digunakan pada proses pemodelan.
- Menjelaskan kelebihan dan kekurangan dari setiap algoritma yang digunakan.
- Jika menggunakan satu algoritma pada solution statement, lakukan proses improvement terhadap model dengan hyperparameter tuning. Jelaskan proses improvement yang dilakukan.
- Jika menggunakan dua atau lebih algoritma pada solution statement, maka pilih model terbaik sebagai solusi. Jelaskan mengapa memilih model tersebut sebagai model terbaik.

### Metrik
Membuat metrik yang akan digunakan untuk evaluasi, menackup nilai accuracy dan *ROC AUC Score* dengan mendefinisikan function.
"""

from sklearn.metrics import accuracy_score, roc_auc_score, matthews_corrcoef

# Siapkan dataframe untuk analisis model
import time
model_performance = pd.DataFrame(columns=['Accuracy', 'ROC AUC score','MCC score','time to train','time to predict','total time'])
list(model_performance)

# Commented out IPython magic to ensure Python compatibility.
# funsi untuk training sekaligus menyimpan nilai metrik
def train_model(algorith, algorith_name, x_train, x_test, y_train):
#   %time
  start = time.time()
  model = algorith.fit(x_train, y_train)
  end_train = time.time()
  y_predictions = model.predict(x_test)
  end_predict = time.time()

  accuracy = accuracy_score(y_test, y_predictions)
  MCC = matthews_corrcoef(y_test, y_predictions)
  ROC_AUC = roc_auc_score(y_test, y_predictions, average='weighted')

  print("Accuracy: "+ "{:.2%}".format(accuracy))
  print("MCC: "+ "{:.2%}".format(MCC))
  print("ROC AUC score: "+ "{:.2%}".format(ROC_AUC))
  print("time to train: "+ "{:.2f}".format(end_train-start)+" s")
  print("time to predict: "+"{:.2f}".format(end_predict-end_train)+" s")
  print("total: "+"{:.2f}".format(end_predict-start)+" s")
  model_performance.loc[algorith_name] = [accuracy, ROC_AUC,MCC,end_train-start,end_predict-end_train,end_predict-start]

"""### KNeighborsClassifier"""

from sklearn.metrics import mean_squared_error
from sklearn.neighbors import KNeighborsClassifier

knc = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)

train_model(algorith=knc, algorith_name='KNC', x_train=X_train, x_test=X_test, y_train=y_train)

"""### RandomForestClassifier


"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()
train_model(algorith=rfc, algorith_name='RFC', x_train=X_train, x_test=X_test, y_train=y_train)

"""### Support Vector Classification"""

from sklearn.svm import SVC

svc = SVC(probability=True)

train_model(algorith=svc, algorith_name='SVC', x_train=X_train, x_test=X_test, y_train=y_train)

"""### GaussianNB"""

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()

train_model(algorith=gnb, algorith_name='GaussianNB', x_train=X_train, x_test=X_test, y_train=y_train)

"""### GradientBoostingClassifier"""

from sklearn.ensemble import GradientBoostingClassifier

boosting = GradientBoostingClassifier(learning_rate=0.05, random_state=55)

train_model(algorith=boosting, algorith_name='GradientBoostingClassifier', x_train=X_train, x_test=X_test, y_train=y_train)

model_performance

"""## Evaluation
- Menyebutkan metrik evaluasi yang digunakan.
- Menjelaskan hasil proyek berdasarkan metrik evaluasi.
- Metrik evaluasi yang digunakan harus sesuai dengan konteks data, problem statement, dan solusi yang diinginkan.
- Menjelaskan metrik evaluasi yang digunakan untuk mengukur kinerja model. Misalnya, menjelaskan formula metrik dan bagaimana metrik tersebut bekerja.

### Accuracy
Merupakan nilai yang didapatlan dari pembagian dari jumlah prediksi yang benar dengan jumlah total prediksi dengan rentang 0-1 atau diubah ke persentase 0-100%.

![rumus accuracy](https://cdn-images-1.medium.com/max/800/1*R6jP_uvlkcxtQSa264N3Sw.png)

Source by [[analyticsvidhya](https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions)]

Metrik akurasi memiliki keterbatasan dimana ketika digunakan untuk data Imbalance maka dapat menimbulkan kesalahan [[towardsdatascience](https://towardsdatascience.com/8-metrics-to-measure-classification-performance-984d9d7fd7aa)].

### ROC AUC
Kurva ROC sangat cocok digunakan untuk data yang imbalance. AUC merupakan nilai yang berada dibawah kurva TPR(*true positive rate*) - FPR (*false positive rate*) [[analyticsvidhya](https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions)].

![grafik roc](https://cdn-images-1.medium.com/max/800/1*bpjCSt38NydElzPf6O5Xng.png)

Source by [analyticsvidhya](https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions)

# MCC(Matthews Correlation Coefficient)
MCC dikenal sebagai salah satu metode terkenal dalam menilai performa suatu model klasifikasi. Metrik ini berfungsi sebagai koefisien korelasi yang menggambarkan sejauh mana hubungan antara hasil klasifikasi yang teramati dengan hasil klasifikasi yang diprediksi. Rentang nilai MCC mirip dengan koefisien korelasi lainnya, yaitu antara -1,0 hingga +1,0. Apabila nilai MCC mencapai +1, hal ini menunjukkan bahwa model memiliki performa yang optimal atau sempurna [[medium](https://towardsdatascience.com/8-metrics-to-measure-classification-performance-984d9d7fd7aa)].

![mcc](https://miro.medium.com/v2/resize:fit:786/format:webp/1*ju2jMm4IsJmQzjYU8l8Mig.png)

source by [medium](https://towardsdatascience.com/8-metrics-to-measure-classification-performance-984d9d7fd7aa)
"""

model_performance.fillna(.90,inplace=True)
model_performance.style.background_gradient(cmap='coolwarm').format({'Accuracy': '{:.2%}',
                                                                     'ROC AUC score': '{:.2%}',
                                                                     'MCC score': '{:.2%}',
                                                                     'time to train':'{:.1f}',
                                                                     'time to predict':'{:.1f}',
                                                                     'total time':'{:.1f}',
                                                                     })

"""Berdasarkan metrik tersebut, terdapat model yang memiliki akurasi tinggi namun nilai ROC AUC dan MCC yang rendah, dan dikarenakan dataset bersifat imbalance maka matrik untuk rujukan utama yaitu ROC AUC dan MCC, sehingga didapatkan 3 model terbaik adalah KNN, RFC dan GradientBoostingClassifier."""

model_dict = {'KNC': knc, 'RFC': rfc, 'GradientBoostingClassifier': boosting, 'SVC':svc, 'GaussianNB':gnb}

prediksi = X_test.iloc[:-50].copy()
pred_dict = {'y_true':y_test[:-50]}
for name, model in model_dict.items():
  pred_dict['prediksi_'+name] = model.predict(prediksi)
pd.DataFrame(pred_dict)

